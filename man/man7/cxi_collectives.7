'\" t
.\" Automatically generated by Pandoc 3.1.3
.\"
.\" Define V font for inline verbatim, using C font in formats
.\" that render this, and otherwise B font.
.ie "\f[CB]x\f[]"x" \{\
. ftr V B
. ftr VI BI
. ftr VB B
. ftr VBI BI
.\}
.el \{\
. ftr V CR
. ftr VI CI
. ftr VB CB
. ftr VBI CBI
.\}
.TH "cxi_collectives" "7" "2025\-07\-23" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
.hy
.SH Libfabric CXI Accelerated Collectives
.SH Introduction
.PP
The libfabric CXI provider supports HPC hardware-accelerated collectives
through the libfabric fi_collectives API.
This is a subset of collectives, with some extensions.
.PP
The libfabric CXI provider does not support non-accelerated collectives.
.PP
This document describes the specific features of this API.
.SH Overview
.PP
The accelerated collectives feature uses special multicast trees within
the Slingshot fabric to accelerate collective operations.
.PP
These multicast trees utilize the fabric switches as reduce-and-hold
storage for partial reductions that traverse the multicast tree from
leaf endpoints to the root endpoint.
Reduction is performed in-tree during the data transfer, and the count
of the contributions is maintained.
.PP
Leaf endpoints compute their contribution to the collective operation
and immediately send it upstream (rootward) on the multicast tree, where
it is counted/reduced in switch hardware.
The root endpoint computes its contribution to the collective, and then
waits for a full count of leaf contributions.
When the root endpoint receives the full count of N-1 contributions from
the leaves, it completes the reduction in software with its own
contribution, and broadcasts the result through the multicast tree
downstream (leafward) to all the leaf endpoints.
.PP
When there are no complications, each endpoint sends and receives
exactly one packet, and only one reduction is performed in software on
the root endpoint.
.PP
This avoids the delays associated with passing through NICs in a
traditional radix-tree implementation of collectives.
.PP
The benefit is that these \[lq]accelerated\[rq] collectives show much
better scaling performance as the number of endpoints increases.
.SH Requirements
.IP \[bu] 2
HPE CSM/HPCM environment
.RS 2
.IP \[bu] 2
Cassini NICs
.IP \[bu] 2
Rosetta fabric switches
.IP \[bu] 2
Shasta Fabric Manager REST API (FM API)
.IP \[bu] 2
Supported Workload Manager (WLM)
.IP \[bu] 2
libfabric/cxi libraries
.IP \[bu] 2
libcurl.so library
.IP \[bu] 2
libjson-c.so library
.RE
.PP
Note: \f[I]The libcurl.so and libjson-c.so libraries must be present,
but will be dynamically loaded into the collective application at
runtime the first time libcurl and libjson routines are needed.
Specifically, libcurl.so and libjson-c.so must be present on any
endpoint that serves as rank 0 for any call to fi_join_collective().
If they are not present, the join will fail.\f[R]
.SH Basic Application Overview
.IP "1." 3
The user application must be started on multiple compute nodes by an
appropriate Workload Manager, such as SLURM or PBS/PALS, which is
adapted to support accelerated collective requirements.
The WLM must:
.RS 4
.IP \[bu] 2
Gain secure access to the fabric manager (HTTPS) prior to job start
.IP \[bu] 2
Generate environment variables needed by the libfabric library
.IP \[bu] 2
Gain secure access to the fabric manager (HTTPS) upon job completion
.RE
.IP "2." 3
User applications must enable collectives for all CXI endpoints (NICs)
to be used in a collective using the \f[V]FI_COLLECTIVE\f[R] flag when
the endpoint is enabled.
.IP "3." 3
User applications must create one or more collective groups using
\f[V]fi_join_collective()\f[R], which will return an mc_obj pointer to
each endpoint that identifies the collective group.
.IP "4." 3
User applications can now use \f[V]fi_barrier()\f[R],
\f[V]fi_broadcast()\f[R], \f[V]fi_reduce()\f[R], and
\f[V]fi_allreduce()\f[R] on these joined collective groups.
.IP "5." 3
Upon completion of use, the application should call \f[V]fi_close()\f[R]
on the mc_obj for each collective group.
Note that simply exiting the application (cleanly or with an abort) will
perform preemptive cleanup of all mc_obj objects.
.SH Collective Functions
.SS Collective Join
.PP
\[lq]Joining\[rq] a collective is the process by which a collective
group is created.
Each endpoint in the collective group must \[lq]join\[rq] the collective
before it can participate in the collective.
The join operation itself is a collective, and no endpoint can proceed
from the join until all endpoints in that group have joined.
.PP
\f[B]Note\f[R]: \f[I]libfabric endpoints in the CXI provider represent
NICs, and each NIC can be individually joined to the collective.
MPI applications use the term RANK to represent compute processes, and
these typically outnumber endpoints.
These RANKS must be locally reduced before submitting the partial
results to the fabric endpoint.\f[R]
.PP
The following system-wide considerations apply to joining collectives:
.IP "1." 3
Only endpoints included within a WLM JOB can be joined to a collective.
.IP "2." 3
Collective groups may overlap (i.e.\ an endpoint can belong to multiple
collective groups).
.IP "3." 3
The number of collective groups in a job is limited (see
\f[V]FI_CXI_HWCOLL_ADDRS_PER_JOB\f[R]).
.IP "4." 3
Any endpoint can serve as HWRoot for \f[I]at most\f[R] one collective
group.
.SS \f[V]fi_av\f[R]
.PP
Any libfabric application requires an \f[V]fi_av\f[R] structure to
convert endpoint hardware addresses to libfabric addresses.
There can be multiple \f[V]fi_av\f[R] structures used for different
purposes.
It is also common to have a single \f[V]fi_av\f[R] structure
representing all endpoints in a job.
This follows the standard libfabric documentation.
.SS \f[V]fi_av_set\f[R]
.PP
Joining a collective requires an \f[V]fi_av_set\f[R] structure that
defines the endpoints to be included in the collective group, which in
turn requires an \f[V]fi_av\f[R] structure that defines all endpoints to
be used in that set.
This follows the standard libfabric documentation.
.IP
.nf
\f[C]
int fi_av_set(cxit_av, &attr, &setp, ctx);
\f[R]
.fi
.IP \[bu] 2
\f[V]cxit_av\f[R] is the \f[V]fi_av\f[R] structure for this job
.IP \[bu] 2
\f[V]attr\f[R] is a custom attribute (\f[V]comm_key\f[R]) structure for
the endpoints
.IP \[bu] 2
\f[V]setp\f[R] is the \f[V]fid_av_set\f[R] pointer for the result
.IP \[bu] 2
\f[V]ctx\f[R] is an optional pointer associated with this operation to
allow \f[V]av_set\f[R] creation concurrency, and can be NULL
.PP
The only cxi-unique feature for this operation is the
\f[V]struct cxip_comm_key\f[R].
This appears in the \f[V]attr\f[R] structure, and should be initialized
to zero.
.IP
.nf
\f[C]
    // clear comm_key structure
    memset(&comm_key, 0, sizeof(comm_key);

    // attributes to create empty av_set
    struct fi_av_set_attr attr = {
        .count = 0,
        .start_addr = FI_ADDR_NOTAVAIL,
        .end_addr = FI_ADDR_NOTAVAIL,
        .stride = 1,
        .comm_key_size = sizeof(comm_key),
        .comm_key = (void *)&comm_key,
        .flags = 0,
    };

    // create empty av_set
    ret = fi_av_set(cxit_av, &attr, &setp, NULL);
    if (ret) {
        fprintf(stderr, \[dq]fi_av_set failed %d\[rs]n\[dq], ret);
        goto quit;
    }

    // append count addresses to av_set
    for (i = 0; i < count; i++) {
        ret = fi_av_set_insert(setp, fiaddrs[i]);
        if (ret) {
            fprintf(stderr, \[dq]fi_av_set_insert failed %d\[rs]n\[dq], ret);
            goto quit;
        }
    }
\f[R]
.fi
.PP
Note: \f[I]The \f[VI]fi_av_set\f[I] endpoints within the structure must
be identical and must appear in the same order on all endpoints.
If the content or ordering differs, results are undefined.\f[R]
.SS \f[V]fi_join_collective()\f[R]
.PP
Once the \f[V]fi_av_set\f[R] structure is created,
\f[V]fi_join_collective()\f[R] can be called to create the collective
mc_obj that represents the multicast tree.
.IP
.nf
\f[C]
int fi_join_collective(ep, FI_ADDR_NOTAVAIL, avset, 0L, &mc_obj, ctx);
\f[R]
.fi
.IP \[bu] 2
\f[V]ep\f[R] is the endpoint on which the function is called
.IP \[bu] 2
\f[V]FI_ADDR_NOTAVAIL\f[R] is a mandatory placeholder
.IP \[bu] 2
\f[V]avset\f[R] is the fi_av_set created above
.IP \[bu] 2
\f[V]flags\f[R] are not supported
.IP \[bu] 2
\f[V]mc_obj\f[R] is the return multicast object pointer
.IP \[bu] 2
\f[V]ctx\f[R] is an arbitrary pointer associated with this operation to
allow concurrency, and can be NULL
.PP
Note: \f[V]fi_join_collective()\f[R] must be called on all endpoints in
the collective with identical av_set structure, or results are
undefined.
.PP
The join operation is asynchronous, and the application must poll the EQ
(Event Queue) to progress the operation and to obtain the result.
Joins are non-concurrent and return \f[V]FI_EAGAIN\f[R] until an active
join completes.
.PP
Note: Internal resource constraints may cause
\f[V]fi_join_collective()\f[R] to return \f[V]-FI_EAGAIN\f[R], and the
operation should be retried after polling the EQ at least once to
progress the running join operations.
.SS Collective Operations
.PP
All collective operations are asynchronous and must be progressed by
polling the CQ (Completion Queue).
.PP
Only eight concurrent reductions can be performed on a given multicast
tree.
Attempts to exceed this limit will result in the \f[V]-FI_EAGAIN\f[R]
error, and the operation should be retried after polling the CQ at least
once.
.PP
All collective operations below are syntactic variants based on
\f[V]fi_allreduce()\f[R], which is the only operation supported by
accelerated collectives.
.SS Barrier
.IP
.nf
\f[C]
ssize_t fi_barrier(struct fid_ep *ep, fi_addr_t coll_addr, void *context)
\f[R]
.fi
.IP \[bu] 2
\f[V]ep\f[R] is the endpoint on which the function is called
.IP \[bu] 2
\f[V]coll_addr\f[R] is the typecast mc_obj for the collective group
.IP \[bu] 2
\f[V]context\f[R] is a user context pointer
.PP
This operation initiates a barrier operation and returns immediately.
The user must poll the CQ for a successful completion.
.PP
It is implemented as an allreduce with no data.
.SS Broadcast
.IP
.nf
\f[C]
ssize_t fi_broadcast(struct fid_ep *ep, void *buf, size_t count,
               void *desc, fi_addr_t coll_addr, fi_addr_t root_addr,
               enum fi_datatype datatype, uint64_t flags,
               void *context)
\f[R]
.fi
.IP \[bu] 2
\f[V]ep\f[R] is the endpoint on which the function is called
.IP \[bu] 2
\f[V]buf\f[R] is the buffer to be sent/received
.IP \[bu] 2
\f[V]count\f[R] is the data count
.IP \[bu] 2
\f[V]desc\f[R] is ignored
.IP \[bu] 2
\f[V]coll_addr\f[R] is the typecast mc_obj for the collective group
.IP \[bu] 2
\f[V]root_addr\f[R] is the address of the designated broadcast root
.IP \[bu] 2
\f[V]datatype\f[R] is the data type
.IP \[bu] 2
\f[V]flags\f[R] modify the operation (see below)
.IP \[bu] 2
\f[V]context\f[R] is a user context pointer
.PP
This operation initiates delivery of the data supplied by the designated
\f[V]root_addr\f[R] to all endpoints.
.PP
It is implemented as an allreduce using the bitwise OR operator.
The data provided in \f[V]buf\f[R] is used on the \f[V]root_addr\f[R]
endpoint, and zero is used on all other endpoints.
.PP
Upon completion, \f[V]buf\f[R] on every endpoint will contain the
contents of \f[V]buf\f[R] from the designated \f[V]root_addr\f[R].
.PP
Note: \f[V]data\f[R] is limited to 16 bytes.
.SS Reduce
.IP
.nf
\f[C]
ssize_t fi_reduce(struct fid_ep *ep, const void *buf, size_t count,
                void *desc, void *result, void *result_desc,
                fi_addr_t coll_addr, fi_addr_t root_addr,
                enum fi_datatype datatype, enum fi_op op,
                uint64_t flags, void *context)
\f[R]
.fi
.IP \[bu] 2
\f[V]ep\f[R] is the endpoint on which the function is called
.IP \[bu] 2
\f[V]buf\f[R] is the buffer to be sent
.IP \[bu] 2
\f[V]count\f[R] is the data count
.IP \[bu] 2
\f[V]desc\f[R] is ignored
.IP \[bu] 2
\f[V]result\f[R] is the result buffer
.IP \[bu] 2
\f[V]result_desc\f[R] is ignored
.IP \[bu] 2
\f[V]coll_addr\f[R] is the typecast mc_obj for the collective group
.IP \[bu] 2
\f[V]root_addr\f[R] is the address of the result target
.IP \[bu] 2
\f[V]datatype\f[R] is the data type
.IP \[bu] 2
\f[V]fi_op\f[R] is the reduction operator
.IP \[bu] 2
\f[V]flags\f[R] modify the operation (see below)
.IP \[bu] 2
\f[V]context\f[R] is a user context pointer
.PP
This operation initiates reduction of the data supplied in \f[V]buf\f[R]
from all endpoints and delivers the \f[V]result\f[R] in the designated
\f[V]root_addr\f[R].
.PP
It is implemented as an allreduce operation, where the result on all
endpoints other than \f[V]root_addr\f[R] is discarded.
.PP
The \f[V]result\f[R] parameter can be NULL on all endpoints other than
the \f[V]root_addr\f[R] endpoint.
.PP
Note: \f[V]data\f[R] is limited to 16 bytes.
.SS Allreduce
.IP
.nf
\f[C]
ssize_t fi_allreduce(struct fid_ep *ep, const void *buf, size_t count,
                void *desc, void *result, void *result_desc,
                fi_addr_t coll_addr,
                enum fi_datatype datatype, enum fi_op op,
                uint64_t flags, void *context)
\f[R]
.fi
.IP \[bu] 2
\f[V]ep\f[R] is the endpoint on which the function is called
.IP \[bu] 2
\f[V]buf\f[R] is the buffer to be sent/received
.IP \[bu] 2
\f[V]count\f[R] is the data count
.IP \[bu] 2
\f[V]desc\f[R] is ignored
.IP \[bu] 2
\f[V]result\f[R] contains the reduced result on completion
.IP \[bu] 2
\f[V]result_desc\f[R] is ignored
.IP \[bu] 2
\f[V]coll_addr\f[R] is the typecast mc_obj for the collective group
.IP \[bu] 2
\f[V]datatype\f[R] is the data type
.IP \[bu] 2
\f[V]fi_op\f[R] is the reduction operator
.IP \[bu] 2
\f[V]flags\f[R] modify the operation (see below)
.IP \[bu] 2
\f[V]context\f[R] is a user context pointer
.PP
This operation initiates reduction of the data supplied in \f[V]buf\f[R]
from all endpoints and delivers it to the \f[V]result\f[R] on all
endpoints.
.PP
Note: \f[V]data\f[R] is limited to 16 bytes.
.SS Collective flags
.PP
Calling any reduction function normally submits the reduction to the
fabric.
.PP
In collective practice, multiple threads are used on a given compute
node, each representing a separate reduction rank.
One of these ranks is designated the \[lq]captain rank,\[rq] which
pre-reduces data from each of the ranks (including itself) before
initiating the multi-endpoint reduction.
.PP
This local reduction is typically performed using normal C operators,
such as sum, multiply, logical operations, or bitwise operations.
.PP
Accelerated collectives provide two \[lq]novel\[rq] operators, the
\f[V]MINMAXLOC\f[R] operator and the \f[V]REPSUM\f[R] operator.
.PP
To allow these functions to be easily used, the \f[V]FI_MORE\f[R] flag
can be specified for any accelerated collective reduction, which \[en]
as the name suggests \[en] informs the reduction that more data is
expected.
This reduces data (in software) and holds the reduction data without
submitting it to the fabric.
This can be repeated any number of times to continue to accumulate
results.
When a subsequent reduction is then performed without the
\f[V]FI_MORE\f[R] flag, the supplied value is taken as the final
contribution, is locally reduced with the existing reduction data, and
the result is submitted to the fabric for collective reduction across
endpoints.
.PP
This mechanism can be used for any operator, such as \f[V]FI_SUM\f[R],
but this is not generally the most efficient way to do this, since the
normal addition operators are available in C.
.SS Collective operators
.PP
The following reduction operators are supported (maximum count in
parentheses):
.PP
.TS
tab(@);
l l l l l l.
T{
Operator
T}@T{
(u)int8/16/32
T}@T{
int64
T}@T{
uint64
T}@T{
double
T}@T{
minmaxloc
T}
_
T{
BAND
T}@T{
yes*
T}@T{
T}@T{
yes(4)
T}@T{
T}@T{
T}
T{
BXOR
T}@T{
yes*
T}@T{
T}@T{
yes(4)
T}@T{
T}@T{
T}
T{
BOR
T}@T{
yes*
T}@T{
T}@T{
yes(4)
T}@T{
T}@T{
T}
T{
MIN
T}@T{
T}@T{
yes(4)
T}@T{
T}@T{
yes(4)
T}@T{
T}
T{
MAX
T}@T{
T}@T{
yes(4)
T}@T{
T}@T{
yes(4)
T}@T{
T}
T{
SUM
T}@T{
T}@T{
yes(4)
T}@T{
T}@T{
yes(4)
T}@T{
T}
T{
REPSUM
T}@T{
T}@T{
T}@T{
T}@T{
yes(1)
T}@T{
T}
T{
MINMAXLOC
T}@T{
T}@T{
T}@T{
T}@T{
T}@T{
yes(1)
T}
.TE
.PP
Note: * \f[V]BAND\f[R], \f[V]BXOR\f[R], and \f[V]BOR\f[R] do not test to
reject collections of signed 8/16/32 bits, but reduce them as packed
collections of up to 4 \f[V]uint64_t\f[R].
.SS NEW OPERATOR MINMAXLOC
.PP
The \f[V]minmaxloc\f[R] operation performs a minimum and a maximum in a
single operation, returning both the minimum and maximum values, along
with the index of the endpoint that contributed that minimum or maximum.
.PP
It can be used to implement the \f[V]MINLOC\f[R] or \f[V]MAXLOC\f[R]
operations by simply setting the unwanted fields to zero and ignoring
the result
.PP
The \f[V]minmaxloc\f[R] structure is specialized:
.PP
.TS
tab(@);
l l l.
T{
Offset
T}@T{
Field
T}@T{
Data Type
T}
_
T{
0
T}@T{
minval
T}@T{
int64
T}
T{
4
T}@T{
minidx
T}@T{
uint64
T}
T{
8
T}@T{
maxval
T}@T{
int64
T}
T{
12
T}@T{
maxidx
T}@T{
uint64
T}
.TE
.SS NEW OPERATOR REPSUM
.PP
The REPSUM operator uses the REPROBLAS algorithm described below:
.PP
https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-121.pdf
Algorithm 7
.PP
This algorithm provides extended-precision double precision summation,
with associative behavior (summation is order-independent).
.PP
Because the summation occurs within a multicast tree that may take
different paths through the fabric on different runs based on other jobs
that are running and using the fabric, the order of summation within the
reduction cannot be generally predicted or controlled.
The well-known ordering problem of double-precision floating point can
lead to varying results on each run.
.PP
The REPSUM algorithm improves on the accuracy of the summation by
implicitly adding more bits to the computations, but more importantly,
guarantees that all additions are associative, meaning they are
order-independent.
.SS Collective Close
.IP
.nf
\f[C]
int fi_close(struct fid *fid);
\f[R]
.fi
.PP
\f[V]fi_close()\f[R] can be called on the mc_obj file identifier
returned by fi_join_collective.
.PP
If the application does not call this before attempting to exit, the
application on one or more endpoints will likely throw exceptions and
WLM job abort, due to unsynchronized removal of global resources.
.PP
The WLM will perform necessary cleanup of global resources.
.SH Environment variables
.SS Workload Manager Environment
.PP
The following environment variables must be provided by the WLM
(Workload Manager) to enable collectives:
.PP
.TS
tab(@);
lw(40.4n) lw(14.8n) lw(14.8n).
T{
Name
T}@T{
Format
T}@T{
Meaning
T}
_
T{
\f[V]FI_CXI_COLL_JOB_ID\f[R]
T}@T{
integer
T}@T{
WLM job identifier
T}
T{
\f[V]FI_CXI_COLL_JOB_STEP_ID\f[R]
T}@T{
integer
T}@T{
WLM job step identifier
T}
T{
\f[V]FI_CXI_COLL_MCAST_TOKEN\f[R]
T}@T{
string
T}@T{
FM API REST authorization token
T}
T{
\f[V]FI_CXI_COLL_FABRIC_MGR_URL\f[R]
T}@T{
string
T}@T{
FM API REST URL
T}
T{
\f[V]FI_CXI_HWCOLL_ADDRS_PER_JOB\f[R]
T}@T{
integer
T}@T{
maximum quota for mcast addresses
T}
.TE
.SS User Environment
.PP
The following environment variable can be provided by the user
application to control collective behavior.
.PP
.TS
tab(@);
lw(38.9n) lw(14.3n) lw(10.4n) lw(6.5n).
T{
Name
T}@T{
Format
T}@T{
Default
T}@T{
Meaning
T}
_
T{
\f[V]FI_CXI_COLL_RETRY_USEC\f[R]
T}@T{
integer
T}@T{
32000
T}@T{
retry period on dropped packets
T}
.TE
.SH Provider-Specific Error Codes
.PP
Provider-specific error codes are supplied through the normal
\f[V]fi_cq_readerr()\f[R] and \f[V]fi_eq_readerr()\f[R] functions.
.PP
A typical optimization is to use \f[V]fi_*_read()\f[R] with a smaller
buffer, and if this fails with -FI_EAVAIL, to use a larger buffer and
call \f[V]fi_*_readerr()\f[R].
.PP
There are two blocks of errors, found in \f[V]fi_cxi_ext.h\f[R].
.SS Reduction Errors
.PP
Reduction errors are reported through the CQ, which is polled to detect
reduction completion events.
.PP
.TS
tab(@);
lw(30.0n) lw(17.5n) lw(22.5n).
T{
Erro code
T}@T{
Value
T}@T{
Meaning
T}
_
T{
\f[V]FI_CXI_ERRNO_RED_FLT_OVERFLOW\f[R]
T}@T{
1024
T}@T{
double precision value overflow
T}
T{
\f[V]FI_CXI_ERRNO_RED_FLT_INVALID\f[R]
T}@T{
1025
T}@T{
double precision sNAN/inf value
T}
T{
\f[V]FI_CXI_ERRNO_RED_INT_OVERFLOW\f[R]
T}@T{
1026
T}@T{
reproducible sum overflow
T}
T{
\f[V]FI_CXI_ERRNO_RED_CONTR_OVERFLOW\f[R]
T}@T{
1027
T}@T{
reduction contribution overflow
T}
T{
\f[V]FI_CXI_ERRNO_RED_OP_MISMATCH\f[R]
T}@T{
1028
T}@T{
reduction opcode mismatch
T}
T{
\f[V]FI_CXI_ERRNO_RED_MC_FAILURE\f[R]
T}@T{
1029
T}@T{
unused
T}
T{
\f[V]FI_CXI_ERRNO_RED_OTHER\f[R]
T}@T{
1030
T}@T{
non-specific reduction error, fatal
T}
.TE
.SS Join Errors
.PP
Join errors are reported through the EQ, which is polled to detect
collective join completion events.
.PP
.TS
tab(@);
lw(31.4n) lw(16.9n) lw(21.7n).
T{
Error code
T}@T{
Value
T}@T{
Meaning
T}
_
T{
\f[V]FI_CXI_ERRNO_JOIN_MCAST_INUSE\f[R]
T}@T{
2048
T}@T{
endpoint already using mcast address
T}
T{
\f[V]FI_CXI_ERRNO_JOIN_HWROOT_INUSE\f[R]
T}@T{
2049
T}@T{
endpoint already serving as HWRoot
T}
T{
\f[V]FI_CXI_ERRNO_JOIN_MCAST_INVALID\f[R]
T}@T{
2050
T}@T{
mcast address from FM is invalid
T}
T{
\f[V]FI_CXI_ERRNO_JOIN_HWROOT_INVALID\f[R]
T}@T{
2051
T}@T{
HWRoot address from FM is invalid
T}
T{
\f[V]FI_CXI_ERRNO_JOIN_CURL_FAILED\f[R]
T}@T{
2052
T}@T{
libcurl initiation failed
T}
T{
\f[V]FI_CXI_ERRNO_JOIN_CURL_TIMEOUT\f[R]
T}@T{
2053
T}@T{
libcurl timed out
T}
T{
\f[V]FI_CXI_ERRNO_JOIN_SERVER_ERR\f[R]
T}@T{
2054
T}@T{
unhandled CURL response code
T}
T{
\f[V]FI_CXI_ERRNO_JOIN_FAIL_PTE\f[R]
T}@T{
2055
T}@T{
libfabric PTE allocation failed
T}
T{
\f[V]FI_CXI_ERRNO_JOIN_OTHER\f[R]
T}@T{
2056
T}@T{
non-specific JOIN error, fatal
T}
.TE
.SH AUTHORS
OpenFabrics.
