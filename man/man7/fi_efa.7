.\" Automatically generated by Pandoc 3.1.3
.\"
.\" Define V font for inline verbatim, using C font in formats
.\" that render this, and otherwise B font.
.ie "\f[CB]x\f[]"x" \{\
. ftr V B
. ftr VI BI
. ftr VB B
. ftr VBI BI
.\}
.el \{\
. ftr V CR
. ftr VI CI
. ftr VB CB
. ftr VBI CBI
.\}
.TH "fi_efa" "7" "2025\-08\-11" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
.hy
.SH NAME
.PP
fi_efa - The Amazon Elastic Fabric Adapter (EFA) Provider
.SH OVERVIEW
.PP
The EFA provider supports the Elastic Fabric Adapter (EFA) device on
Amazon EC2.
EFA provides reliable and unreliable datagram send/receive with direct
hardware access from userspace (OS bypass).
For reliable datagram (RDM) EP type, it supports two fabric names:
\f[V]efa\f[R] and \f[V]efa-direct\f[R].
The \f[V]efa\f[R] fabric implements a set of wire protocols to support
more capabilities and features beyond the EFA device capabilities.
The \f[V]efa-direct\f[R] fabric, on the contrary, offloads all libfabric
data plane calls to the device directly without wire protocols.
Compared to the \f[V]efa\f[R] fabric, the \f[V]efa-direct\f[R] fabric
supports fewer capabilities and has more mode requirements for
applications.
But it provides a fast path to hand off application requests to the
device.
To use \f[V]efa-direct\f[R], set the name field in
\f[V]fi_fabric_attr\f[R] to \f[V]efa-direct\f[R].
More details and difference between the two fabrics will be presented
below.
.SH SUPPORTED FEATURES
.PP
The following features are supported:
.TP
\f[I]Endpoint types\f[R]
The provider supports endpoint type \f[I]FI_EP_DGRAM\f[R], and
\f[I]FI_EP_RDM\f[R] on a new Scalable (unordered) Reliable Datagram
protocol (SRD).
SRD provides support for reliable datagrams and more complete error
handling than typically seen with other Reliable Datagram (RD)
implementations.
.TP
\f[I]RDM Endpoint capabilities\f[R]
For the \f[V]efa\f[R] fabric, the following data transfer interfaces are
supported: \f[I]FI_MSG\f[R], \f[I]FI_TAGGED\f[R], \f[I]FI_SEND\f[R],
\f[I]FI_RECV\f[R], \f[I]FI_RMA\f[R], \f[I]FI_WRITE\f[R],
\f[I]FI_READ\f[R], \f[I]FI_ATOMIC\f[R], \f[I]FI_DIRECTED_RECV\f[R],
\f[I]FI_MULTI_RECV\f[R], and \f[I]FI_SOURCE\f[R].
It provides SAS guarantees for data operations, and does not have a
maximum message size (for all operations).
For the \f[V]efa-direct\f[R] fabric, it supports \f[I]FI_MSG\f[R],
\f[I]FI_SEND\f[R], \f[I]FI_RECV\f[R], \f[I]FI_RMA\f[R],
\f[I]FI_WRITE\f[R], \f[I]FI_READ\f[R], and \f[I]FI_SOURCE\f[R].
As mentioned earlier, it doesn\[cq]t provide SAS guarantees, and has
different maximum message sizes for different operations.
For MSG operations, the maximum message size is the MTU size of the efa
device (approximately 8KiB).
For RMA operations, the maximum message size is the maximum RDMA size of
the EFA device.
The exact values of these sizes can be queried by the
\f[V]fi_getopt\f[R] API with option names \f[V]FI_OPT_MAX_MSG_SIZE\f[R]
and \f[V]FI_OPT_MAX_RMA_SIZE\f[R]
.TP
\f[I]DGRAM Endpoint capabilities\f[R]
The DGRAM endpoint only supports \f[I]FI_MSG\f[R] capability with a
maximum message size of the MTU of the underlying hardware
(approximately 8 KiB).
.TP
\f[I]Address vectors\f[R]
The provider supports \f[I]FI_AV_TABLE\f[R].
\f[I]FI_AV_MAP\f[R] was deprecated in Libfabric 2.x.
Applications can still use \f[I]FI_AV_MAP\f[R] to create an address
vector.
But the EFA provider implementation will print a warning and switch to
\f[I]FI_AV_TABLE\f[R].
\f[I]FI_EVENT\f[R] is unsupported.
.TP
\f[I]Completion events\f[R]
The provider supports \f[I]FI_CQ_FORMAT_CONTEXT\f[R],
\f[I]FI_CQ_FORMAT_MSG\f[R], and \f[I]FI_CQ_FORMAT_DATA\f[R].
\f[I]FI_CQ_FORMAT_TAGGED\f[R] is supported on the \f[V]efa\f[R] fabric
of RDM endpoint.
Wait objects are not currently supported.
.TP
\f[I]Modes\f[R]
The provider requires the use of \f[I]FI_MSG_PREFIX\f[R] when running
over the DGRAM endpoint.
And it requires the use of \f[I]FI_CONTEXT2\f[R] mode for DGRAM endpoint
and the \f[V]efa-direct\f[R] fabric of RDM endpoint.
The \f[V]efa\f[R] fabric of RDM endpoint doesn\[cq]t have these
requirements.
.TP
\f[I]Memory registration modes\f[R]
The \f[V]efa\f[R] fabric of RDM endpoint does not require memory
registration for send and receive operations, i.e.\ it does not require
\f[I]FI_MR_LOCAL\f[R].
Applications may specify \f[I]FI_MR_LOCAL\f[R] in the MR mode flags in
order to use descriptors provided by the application.
The \f[V]efa-direct\f[R] fabric of \f[I]FI_EP_RDM\f[R] endpint and the
\f[I]FI_EP_DGRAM\f[R] endpoint only supports \f[I]FI_MR_LOCAL\f[R].
.TP
\f[I]Progress\f[R]
RDM and DGRAM endpoints support \f[I]FI_PROGRESS_MANUAL\f[R].
EFA erroneously claims the support for \f[I]FI_PROGRESS_AUTO\f[R],
despite not properly supporting automatic progress.
Unfortunately, some Libfabric consumers also ask for
\f[I]FI_PROGRESS_AUTO\f[R] when they only require
\f[I]FI_PROGRESS_MANUAL\f[R], and fixing this bug would break those
applications.
This will be fixed in a future version of the EFA provider by adding
proper support for \f[I]FI_PROGRESS_AUTO\f[R].
.TP
\f[I]Threading\f[R]
Both RDM and DGRAM endpoints supports \f[I]FI_THREAD_SAFE\f[R].
.SH LIMITATIONS
.SS Completion events
.IP \[bu] 2
Synchronous CQ read is not supported.
.IP \[bu] 2
Wait objects are not currently supported.
.SS RMA operations
.IP \[bu] 2
Completion events for RMA targets (\f[I]FI_RMA_EVENT\f[R]) is not
supported.
.IP \[bu] 2
For the \f[V]efa-direct\f[R] fabric, the target side of RMA operation
must insert the initiator side\[cq]s address into AV before the RMA
operation is kicked off, due to a current device limitation.
The same limitation applies to the \f[V]efa\f[R] fabric when the
\f[V]FI_OPT_EFA_HOMOGENEOUS_PEERS\f[R] option is set as \f[V]true\f[R].
.SS Zero-copy receive mode
.IP \[bu] 2
Zero-copy receive mode can be enabled only if SHM transfer is disabled.
.IP \[bu] 2
Unless the application explicitly disables P2P, e.g.\ via
FI_HMEM_P2P_DISABLED, zero-copy receive can be enabled only if available
FI_HMEM devices all have P2P support.
.SS \f[V]fi_cancel\f[R] support
.IP \[bu] 2
\f[V]fi_cancel\f[R] is only supported in the non-zero-copy-receive mode
of the \f[V]efa\f[R] fabric.
It\[cq]s not supported in \f[V]efa-direct\f[R], DGRAM endpoint, and the
zero-copy receive mode of the \f[V]efa\f[R] fabric in RDM endpoint.
.PP
When using FI_HMEM for AWS Neuron or Habana SynapseAI buffers, the
provider requires peer to peer transaction support between the EFA and
the FI_HMEM device.
Therefore, the FI_HMEM_P2P_DISABLED option is not supported by the EFA
provider for AWS Neuron or Habana SynapseAI.
.SH PROVIDER SPECIFIC ENDPOINT LEVEL OPTION
.TP
\f[I]FI_OPT_EFA_RNR_RETRY\f[R]
Defines the number of RNR retry.
The application can use it to reset RNR retry counter via the call to
fi_setopt.
Note that this option must be set before the endpoint is enabled.
Otherwise, the call will fail.
Also note that this option only applies to RDM endpoint.
.TP
\f[I]FI_OPT_EFA_EMULATED_READ, FI_OPT_EFA_EMULATED_WRITE, FI_OPT_EFA_EMULATED_ATOMICS - bool\f[R]
These options only apply to the fi_getopt() call.
They are used to query the EFA provider to determine if the endpoint is
emulating Read, Write, and Atomic operations (return value is true), or
if these operations are assisted by hardware support (return value is
false).
.TP
\f[I]FI_OPT_EFA_USE_DEVICE_RDMA - bool\f[R]
This option only applies to the fi_setopt() call.
Only available if the application selects a libfabric API version >=
1.18.
This option allows an application to change libfabric\[cq]s behavior
with respect to RDMA transfers.
Note that there is also an environment variable FI_EFA_USE_DEVICE_RDMA
which the user may set as well.
If the environment variable and the argument provided with this variable
are in conflict, then fi_setopt will return -FI_EINVAL, and the
environment variable will be respected.
If the hardware does not support RDMA and the argument is true, then
fi_setopt will return -FI_EOPNOTSUPP.
If the application uses API version < 1.18, the argument is ignored and
fi_setopt returns -FI_ENOPROTOOPT.
The default behavior for RDMA transfers depends on API version.
For API >= 1.18 RDMA is enabled by default on any hardware which
supports it.
For API<1.18, RDMA is enabled by default only on certain newer hardware
revisions.
.TP
\f[I]FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES - bool\f[R]
This option only applies to the fi_setopt() call.
It is used to force the endpoint to use in-order send/recv operation for
each 128 bytes aligned block.
Enabling the option will guarantee data inside each 128 bytes aligned
block being sent and received in order, it will also guarantee data to
be delivered to the receive buffer only once.
If endpoint is not able to support this feature, it will return
-FI_EOPNOTSUPP for the call to fi_setopt().
.TP
\f[I]FI_OPT_EFA_WRITE_IN_ORDER_ALIGNED_128_BYTES - bool\f[R]
This option only applies to the fi_setopt() call.
It is used to set the endpoint to use in-order RDMA write operation for
each 128 bytes aligned block.
Enabling the option will guarantee data inside each 128 bytes aligned
block being written in order, it will also guarantee data to be
delivered to the target buffer only once.
If endpoint is not able to support this feature, it will return
-FI_EOPNOTSUPP for the call to fi_setopt().
.TP
\f[I]FI_OPT_EFA_HOMOGENEOUS_PEERS - bool\f[R]
This option only applies to the fi_setopt() call for RDM endpoints on
efa fabric.
RDM endpoints on efa-direct fabric are unaffected by this option.
When set to true, it indicates all peers are homogeneous, meaning they
run on the same platform, use the same software versions, and share
identical capabilities.
It accelerates the initial communication setup as interoperability
between peers is guaranteed.
When set to true, the target side of a RMA operation must insert the
initiator side\[cq]s address into AV before the RMA operation is kicked
off, due to a current device limitation.
The default value is false.
.SH PROVIDER SPECIFIC DOMAIN OPS
.PP
The efa provider exports extensions for operations that are not provided
by the standard libfabric interface.
These extensions are available via the \[lq]\f[V]fi_ext_efa.h\f[R]\[rq]
header file.
.SS Domain Operation Extension
.PP
Domain operation extension is obtained by calling \f[V]fi_open_ops\f[R]
(see \f[V]fi_domain(3)\f[R])
.IP
.nf
\f[C]
int fi_open_ops(struct fid *domain, const char *name, uint64_t flags,
    void **ops, void *context);
\f[R]
.fi
.PP
Requesting \f[V]FI_EFA_DOMAIN_OPS\f[R] in \f[V]name\f[R] returns
\f[V]ops\f[R] as the pointer to the function table
\f[V]fi_efa_ops_domain\f[R] defined as follows:
.IP
.nf
\f[C]
struct fi_efa_ops_domain {
    int (*query_mr)(struct fid_mr *mr, struct fi_efa_mr_attr *mr_attr);
};
\f[R]
.fi
.SS query_mr
.PP
This op queries an existing memory registration as input, and outputs
the efa specific mr attribute which is defined as follows
.IP
.nf
\f[C]
struct fi_efa_mr_attr {
    uint16_t ic_id_validity;
    uint16_t recv_ic_id;
    uint16_t rdma_read_ic_id;
    uint16_t rdma_recv_ic_id;
};
\f[R]
.fi
.TP
\f[I]ic_id_validity\f[R]
Validity mask of interconnect id fields.
Currently the following bits are supported in the mask:
.RS
.PP
FI_EFA_MR_ATTR_RECV_IC_ID: recv_ic_id has a valid value.
.PP
FI_EFA_MR_ATTR_RDMA_READ_IC_ID: rdma_read_ic_id has a valid value.
.PP
FI_EFA_MR_ATTR_RDMA_RECV_IC_ID: rdma_recv_ic_id has a valid value.
.RE
.TP
\f[I]recv_ic_id\f[R]
Physical interconnect used by the device to reach the MR for receive
operation.
It is only valid when \f[V]ic_id_validity\f[R] has the
\f[V]FI_EFA_MR_ATTR_RECV_IC_ID\f[R] bit.
.TP
\f[I]rdma_read_ic_id\f[R]
Physical interconnect used by the device to reach the MR for RDMA read
operation.
It is only valid when \f[V]ic_id_validity\f[R] has the
\f[V]FI_EFA_MR_ATTR_RDMA_READ_IC_ID\f[R] bit.
.TP
\f[I]rdma_recv_ic_id\f[R]
Physical interconnect used by the device to reach the MR for RDMA write
receive.
It is only valid when \f[V]ic_id_validity\f[R] has the
\f[V]FI_EFA_MR_ATTR_RDMA_RECV_IC_ID\f[R] bit.
.SS Return value
.PP
\f[B]query_mr()\f[R] returns 0 on success, or the value of errno on
failure (which indicates the failure reason).
.PP
To enable GPU Direct Async (GDA), which allows the GPU to interact
directly with the NIC, request \f[V]FI_EFA_GDA_OPS\f[R] in the
\f[V]name\f[R] parameter with efa-direct fabirc.
This returns \f[V]ops\f[R] as a pointer to the function table
\f[V]fi_efa_ops_gda\f[R] defined as follows:
.IP
.nf
\f[C]
struct fi_efa_ops_gda {
    int (*query_addr)(struct fid_ep *ep_fid, fi_addr_t addr, uint16_t *ahn,
              uint16_t *remote_qpn, uint32_t *remote_qkey);
    int (*query_qp_wqs)(struct fid_ep *ep_fid, struct fi_efa_wq_attr *sq_attr, struct fi_efa_wq_attr *rq_attr);
    int (*query_cq)(struct fid_cq *cq_fid, struct fi_efa_cq_attr *cq_attr);
    int (*cq_open_ext)(struct fid_domain *domain_fid,
               struct fi_cq_attr *attr,
               struct fi_efa_cq_init_attr *efa_cq_init_attr,
               struct fid_cq **cq_fid, void *context);
    uint64_t (*get_mr_lkey)(struct fid_mr *mr);
};
\f[R]
.fi
.SS query_addr
.PP
This op queries the following address information for a given endpoint
and destination address.
.TP
\f[I]ahn\f[R]
Address handle number.
.TP
\f[I]remote_qpn\f[R]
Remote queue pair Number.
.TP
\f[I]remote_qkey\f[R]
qkey for the remote queue pair.
.SS Return value
.PP
\f[B]query_addr()\f[R] returns FI_SUCCESS on success, or -FI_EINVAL on
failure.
.SS query_qp_wqs
.PP
This op queries EFA specific Queue Pair work queue attributes for a
given endpoint.
It retrieves the send queue attributes in sq_attr and receive queue
attributes in rq_attr, which is defined as follows.
.IP
.nf
\f[C]
struct fi_efa_wq_attr {
    uint8_t *buffer;
    uint32_t entry_size;
    uint32_t num_entries;
    uint32_t *doorbell;
    uint32_t max_batch;
};
\f[R]
.fi
.TP
\f[I]buffer\f[R]
Queue buffer.
.TP
\f[I]entry_size\f[R]
Size of each entry in the queue.
.TP
\f[I]num_entries\f[R]
Maximal number of entries in the queue.
.TP
\f[I]doorbell\f[R]
Queue doorbell.
.TP
\f[I]max_batch\f[R]
Maximum batch size for queue submissions.
.SS Return value
.PP
\f[B]query_qp_wqs()\f[R] returns 0 on success, or the value of errno on
failure (which indicates the failure reason).
.SS query_cq
.PP
This op queries EFA specific Completion Queue attributes for a given cq.
.IP
.nf
\f[C]
struct fi_efa_cq_attr {
    uint8_t *buffer;
    uint32_t entry_size;
    uint32_t num_entries;
};
\f[R]
.fi
.TP
\f[I]buffer\f[R]
Completion queue buffer.
.TP
\f[I]entry_size\f[R]
Size of each completion queue entry.
.TP
\f[I]num_entries\f[R]
Maximal number of entries in the completion queue.
.SS Return value
.PP
\f[B]query_cq()\f[R] returns 0 on success, or the value of errno on
failure (which indicates the failure reason).
.SS cq_open_ext
.PP
This op creates a completion queue with external memory provided via
dmabuf.
The memory can be passed by supplying the following struct.
.IP
.nf
\f[C]
struct fi_efa_cq_init_attr {
    uint64_t flags;
    struct {
        uint8_t *buffer;
        uint64_t length;
        uint64_t offset;
        uint32_t fd;
    } ext_mem_dmabuf;
};
\f[R]
.fi
.TP
\f[I]flags\f[R]
A bitwise OR of the various values described below.
.RS
.PP
FI_EFA_CQ_INIT_FLAGS_EXT_MEM_DMABUF: create CQ with external memory
provided via dmabuf.
.RE
.TP
\f[I]ext_mem_dmabuf\f[R]
Structure containing information about external memory when using
FI_EFA_CQ_INIT_FLAGS_EXT_MEM_DMABUF flag.
.RS
.TP
\f[I]buffer\f[R]
Pointer to the memory mapped in the process\[cq]s virtual address space.
The field is optional, but if not provided, the use of CQ poll
interfaces should be avoided.
.TP
\f[I]length\f[R]
Length of the memory region to use.
.TP
\f[I]offset\f[R]
Offset within the dmabuf.
.TP
\f[I]fd\f[R]
File descriptor of the dmabuf.
.RE
.SS Return value
.PP
\f[B]cq_open_ext()\f[R] returns 0 on success, or the value of errno on
failure (which indicates the failure reason).
.SS get_mr_lkey
.PP
Returns the local memory translation key associated with a MR.
The memory registration must have completed successfully before invoking
this.
.TP
\f[I]lkey\f[R]
local memory translation key used by TX/RX buffer descriptor.
.SS Return value
.PP
\f[B]get_mr_lkey()\f[R] returns lkey on success, or FI_KEY_NOTAVAIL if
the registration has not completed.
.SH Traffic Class (tclass) in EFA
.PP
To prioritize the messages from a given endpoint, user can specify
\f[V]fi_info->tx_attr->tclass = FI_TC_LOW_LATENCY\f[R] in the
fi_endpoint() call to set the service level in rdma-core.
All other tclass values will be ignored.
.SH RUNTIME PARAMETERS
.TP
\f[I]FI_EFA_IFACE\f[R]
A comma-delimited list of EFA device, i.e.\ NIC, names that should be
visible to the application.
This paramater can be used to include/exclude NICs to enforce process
affinity based on the hardware topology.
The default value is \[lq]all\[rq] which allows all available NICs to be
discovered.
.TP
\f[I]FI_EFA_TX_SIZE\f[R]
Maximum number of transmit operations before the provider returns
-FI_EAGAIN.
For only the RDM endpoint, this parameter will cause transmit operations
to be queued when this value is set higher than the default and the
transmit queue is full.
.TP
\f[I]FI_EFA_RX_SIZE\f[R]
Maximum number of receive operations before the provider returns
-FI_EAGAIN.
.SH RUNTIME PARAMETERS SPECIFIC TO RDM ENDPOINT
.PP
These OFI runtime parameters apply only to the RDM endpoint.
.TP
\f[I]FI_EFA_RX_WINDOW_SIZE\f[R]
Maximum number of MTU-sized messages that can be in flight from any
single endpoint as part of long message data transfer.
.TP
\f[I]FI_EFA_TX_QUEUE_SIZE\f[R]
Depth of transmit queue opened with the NIC.
This may not be set to a value greater than what the NIC supports.
.TP
\f[I]FI_EFA_RECVWIN_SIZE\f[R]
Size of out of order reorder buffer (in messages).
Messages received out of this window will result in an error.
.TP
\f[I]FI_EFA_CQ_SIZE\f[R]
Size of any cq created, in number of entries.
.TP
\f[I]FI_EFA_MR_CACHE_ENABLE\f[R]
Enables using the mr cache and in-line registration instead of a bounce
buffer for iov\[cq]s larger than max_memcpy_size.
Defaults to true.
When disabled, only uses a bounce buffer
.TP
\f[I]FI_EFA_MR_MAX_CACHED_COUNT\f[R]
Sets the maximum number of memory registrations that can be cached at
any time.
.TP
\f[I]FI_EFA_MR_MAX_CACHED_SIZE\f[R]
Sets the maximum amount of memory that cached memory registrations can
hold onto at any time.
.TP
\f[I]FI_EFA_MAX_MEMCPY_SIZE\f[R]
Threshold size switch between using memory copy into a pre-registered
bounce buffer and memory registration on the user buffer.
.TP
\f[I]FI_EFA_MTU_SIZE\f[R]
Overrides the default MTU size of the device.
.TP
\f[I]FI_EFA_RX_COPY_UNEXP\f[R]
Enables the use of a separate pool of bounce-buffers to copy unexpected
messages out of the pre-posted receive buffers.
.TP
\f[I]FI_EFA_RX_COPY_OOO\f[R]
Enables the use of a separate pool of bounce-buffers to copy
out-of-order RTS packets out of the pre-posted receive buffers.
.TP
\f[I]FI_EFA_MAX_TIMEOUT\f[R]
Maximum timeout (us) for backoff to a peer after a receiver not ready
error.
.TP
\f[I]FI_EFA_TIMEOUT_INTERVAL\f[R]
Time interval (us) for the base timeout to use for exponential backoff
to a peer after a receiver not ready error.
.TP
\f[I]FI_EFA_ENABLE_SHM_TRANSFER\f[R]
Enable SHM provider to provide the communication across all intra-node
processes.
SHM transfer will be disabled in the case where
\f[V]ptrace protection\f[R] is turned on.
You can turn it off to enable shm transfer.
.PP
FI_EFA_ENABLE_SHM_TRANSFER is parsed during the fi_domain call and is
related to the FI_OPT_SHARED_MEMORY_PERMITTED endpoint option.
If FI_EFA_ENABLE_SHM_TRANSFER is set to true, the
FI_OPT_SHARED_MEMORY_PERMITTED endpoint option overrides
FI_EFA_ENABLE_SHM_TRANSFER.
If FI_EFA_ENABLE_SHM_TRANSFER is set to false, but the
FI_OPT_SHARED_MEMORY_PERMITTED is set to true, the
FI_OPT_SHARED_MEMORY_PERMITTED setopt call will fail with -FI_EINVAL.
.TP
\f[I]FI_EFA_SHM_AV_SIZE\f[R]
Defines the maximum number of entries in SHM provider\[cq]s address
vector.
.TP
\f[I]FI_EFA_SHM_MAX_MEDIUM_SIZE\f[R]
Defines the switch point between small/medium message and large message.
The message larger than this switch point will be transferred with large
message protocol.
NOTE: This parameter is now deprecated.
.TP
\f[I]FI_EFA_INTER_MAX_MEDIUM_MESSAGE_SIZE\f[R]
The maximum size for inter EFA messages to be sent by using medium
message protocol.
Messages which can fit in one packet will be sent as eager message.
Messages whose sizes are smaller than this value will be sent using
medium message protocol.
Other messages will be sent using CTS based long message protocol.
.TP
\f[I]FI_EFA_FORK_SAFE\f[R]
Enable fork() support.
This may have a small performance impact and should only be set when
required.
Applications that require to register regions backed by huge pages and
also require fork support are not supported.
.TP
\f[I]FI_EFA_RUNT_SIZE\f[R]
The maximum number of bytes that will be eagerly sent by inflight
messages uses runting read message protocol (Default 307200).
.TP
\f[I]FI_EFA_INTER_MIN_READ_MESSAGE_SIZE\f[R]
The minimum message size in bytes for inter EFA read message protocol.
If instance support RDMA read, messages whose size is larger than this
value will be sent by read message protocol.
(Default 1048576).
.TP
\f[I]FI_EFA_INTER_MIN_READ_WRITE_SIZE\f[R]
The mimimum message size for emulated inter EFA write to use read write
protocol.
If firmware support RDMA read, and FI_EFA_USE_DEVICE_RDMA is 1, write
requests whose size is larger than this value will use the read write
protocol (Default 65536).
If the firmware supports RDMA write, device RDMA write will always be
used.
.TP
\f[I]FI_EFA_USE_DEVICE_RDMA\f[R]
Specify whether to require or ignore RDMA features of the EFA device.
- When set to 1/true/yes/on, all RDMA features of the EFA device are
used.
But if EFA device does not support RDMA and FI_EFA_USE_DEVICE_RDMA is
set to 1/true/yes/on, user\[cq]s application is aborted and a warning
message is printed.
- When set to 0/false/no/off, libfabric will emulate all fi_rma
operations instead of offloading them to the EFA network device.
Libfabric will not use device RDMA to implement send/receive operations.
- If not set, RDMA operations will occur when available based on RDMA
device ID/version.
.TP
\f[I]FI_EFA_USE_HUGE_PAGE\f[R]
Specify Whether EFA provider can use huge page memory for internal
buffer.
Using huge page memory has a small performance advantage, but can cause
system to run out of huge page memory.
By default, EFA provider will use huge page unless FI_EFA_FORK_SAFE is
set to 1/on/true.
.TP
\f[I]FI_EFA_USE_ZCPY_RX\f[R]
Enables the use of application\[cq]s receive buffers in place of
bounce-buffers when feasible.
(Default: 1).
Setting this environment variable to 0 can disable this feature.
Explicitly setting this variable to 1 does not guarantee this feature
due to other requirements.
See
https://github.com/ofiwg/libfabric/blob/main/prov/efa/docs/efa_rdm_protocol_v4.md
for details.
.TP
\f[I]FI_EFA_USE_UNSOLICITED_WRITE_RECV\f[R]
Use device\[cq]s unsolicited write recv functionality when it\[cq]s
available.
(Default: 1).
Setting this environment variable to 0 can disable this feature.
.TP
\f[I]FI_EFA_INTERNAL_RX_REFILL_THRESHOLD\f[R]
The threshold that EFA provider will refill the internal rx pkt pool.
(Default: 8).
When the number of internal rx pkts to post is lower than this
threshold, the refill will be skipped.
.TP
\f[I]FI_EFA_USE_DATA_PATH_DIRECT\f[R]
Use the direct data path implementation that bypasses rdma-core on data
path, including the CQ polling and TX/RX submissions, when it\[cq]s
available.
Setting this variable as 1 will enable this feature (Default: false).
.SH SEE ALSO
.PP
\f[V]fabric\f[R](7), \f[V]fi_provider\f[R](7), \f[V]fi_getinfo\f[R](3)
.SH AUTHORS
OpenFabrics.
